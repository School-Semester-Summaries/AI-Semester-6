{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake - Kaan Gogcay\n",
    "In this notebook I made an agent that can learn how to navigate from start to finish. When creating a new enivornment with a new map layout, it will need to relearn everything. Everything it learn is based of the current map layout. Througout the document I will try to explain what is going on.\n",
    "- [Import Depencies](#import-dependencies)\n",
    "- [Create Environment](#create-environment)\n",
    "- [Define Q-Table](#define-q-table)\n",
    "- [Define Agent](#define-agent)\n",
    "- [Create Agent](#create-agent)\n",
    "- [Make the Agent Learn](#make-the-agent-learn)\n",
    "- [Test Agent](#test-agent)\n",
    "- [Evalutaion](#evaluation)\n",
    "- [Sources](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.18 \n",
      "0.26.2\n",
      "2.0.3\n",
      "1.23.4\n"
     ]
    }
   ],
   "source": [
    "import sys # 3.8.18\n",
    "import gym # 0.26.2\n",
    "import pandas as pd # 2.0.3\n",
    "import numpy as np # 1.23.4\n",
    "import random as rnd # N/A\n",
    "import time # N/A\n",
    "\n",
    "print(sys.version[0:7])\n",
    "print(gym.__version__)\n",
    "print(pd.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment\n",
    "when creating the environment you can see a few noticeable things.\n",
    "- `is_slippery=False` - `is_slippery` makes it possible that the agent will move either 90 degrees left or 90 degrees right. For example, if it wants to move right. the actual chance of moving right is 1/3, the chance of moving up is 1/3 (90 degrees to the left), and the chance to move down is also 1/3 (90 degrees to the right). I decided to set this to False since it adds a lot of chance to the exercise. I'm trying to get into RL and don't want to make it too overcomplicated right from the start.\n",
    "- `map_name='8x8'` - this makes the map 8x8 instead of 4x4. By making the map bigger it is less easy to reach the gift by accident. Meaning it requires a well thought reward system to make the Agent learn how to play.\n",
    "- `desc=[x]` - here I set the layout of the map. In this map there are multiple ways to reach the gift mixed with lots of holes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_base=[\n",
    "\"SFFFFFFF\",\n",
    "\"HHHHHHHF\",\n",
    "\"FFFFFFHF\",\n",
    "\"FHHHHFHF\",\n",
    "\"FHFGHFHF\",\n",
    "\"FHFFFFHF\",\n",
    "\"FHHHHHHF\",\n",
    "\"FFFFFFFF\"\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: 64\n",
      "actions: 4\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make('FrozenLake-v1', render_mode='human', is_slippery = False, map_name='8x8')\n",
    "env = gym.make('FrozenLake-v1', \n",
    "               is_slippery=False, \n",
    "               map_name='8x8', \n",
    "               desc=desc_base)\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print(f'states: {env.observation_space.n}')\n",
    "print(f'actions: {env.action_space.n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Q-Table\n",
    "To make the agent learn it will need something to base its decisions off of. Therefore I'm creating a Q-Table. This is basically a very long array with states. foreach state array, we have an array with actions it can take. and for each state-action combination there will be a reward saved. this way it will slowly find out which action gives the best reward in a certain state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigger_q_table(states, actions):\n",
    "    q = np.full((states, actions), 0) # 16 arrays\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent\n",
    "Here we actually define the Agent. The agent has a few functions worth mentioning.\n",
    "- `step(self, state, exploration_rate)` - step() evantually returns an action. before it returns an action, it checks which action gives the highest reward. If there are multiple actions giving the same reward, it will make a random choice between the good actions.\n",
    "- `learn(self, state, action, reward)` - learn() overwrites the reward gained for a state-action combination if the reward is higher than the one saved previously. when we define the Q-Table, every state-action combination will have a reward of 0 saved. so whenever we give a reward bigger than 0 it will overwrite the default reward. This also means that if ew give a reward below 0 it will not overwrite. This gets usefull whenever we set the `exploration_rate` high.\n",
    "- `force_learn(self, state, action, reward)` - force_learn() will always overwrite a reward. It won't check if the previously saved reward is better. it will force overwrite the reward onto the state-action combination. This is usefull for whenever we want to punish our agent. since the default value of each reward is 0. we can't overwrite it with the learn() function.\n",
    "- `evaluate(self)` - prints the Q-Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_v6():  \n",
    "    def __init__(self, q_table):\n",
    "        self.q_table = q_table\n",
    "\n",
    "\n",
    "    def step(self, state, exploration_rate):\n",
    "        # Random action if exploring\n",
    "        if rnd.randint(1, 100) <= (exploration_rate*100):\n",
    "            return env.action_space.sample()\n",
    "        \n",
    "        # Find best action\n",
    "        actions = self.q_table[state]\n",
    "        best_reward = [-1.0]\n",
    "        good_actions = []\n",
    "        for i, r in enumerate(actions):\n",
    "            if r > best_reward:\n",
    "                best_reward = r\n",
    "                good_actions.clear()\n",
    "            if r == best_reward:\n",
    "                ('add it')\n",
    "                good_actions.append(i)\n",
    "        choice = rnd.choice(good_actions)\n",
    "        return choice\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, state, action, reward):\n",
    "        old_reward = self.q_table[state, action]\n",
    "        if reward > old_reward:\n",
    "            self.q_table[state, action] = reward\n",
    "\n",
    "\n",
    "    def force_learn(self, state, action, reward):\n",
    "        self.q_table[state, action] = reward\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        df = pd.DataFrame(self.q_table)\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent\n",
    "Here we combine everything we mentioned above. We create the Agent and give him a Q-Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent11 = Agent_v6(create_bigger_q_table(env.observation_space.n, env.action_space.n))\n",
    "agent11.q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Agent Learn\n",
    "To make the agent learn we will let it play in the game loop I've created. Inside this gameloop there is happening a lot. I tried to write comments to make it as clear as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "max_moves = 1000 # max moves before resetting env\n",
    "episodes = 70 # amouint of tries\n",
    "exploration_rate=0.0 # the chance of ignoring what we've learned and doing a random action\n",
    "steps_looking_into_future = 36 # the amount of steps we should look into the future\n",
    "agent = agent11 # set agent\n",
    "\n",
    "for episode in range(episodes):\n",
    "    new_observation = 0  # We consider that we always start top left // if you wanna start somewhere else you will have to change this variable \n",
    "    action_history = [] # clear the action history\n",
    "    state_history = [] # clear the state history\n",
    "    env.reset()\n",
    "    for move in range(max_moves):  \n",
    "\n",
    "        # keep track of the action history\n",
    "        # later used to laern the entire state-action history if we ever reach the gift\n",
    "        if move>0:\n",
    "            if len(action_history) < steps_looking_into_future:\n",
    "                action_history.append(action)\n",
    "            else: \n",
    "                action_history.pop(0)\n",
    "                action_history.append(action)\n",
    "\n",
    "        # Get an action based of the Q-table\n",
    "        action = agent.step(new_observation, exploration_rate)\n",
    "\n",
    "        # keep track of the state history\n",
    "        # later used to laern the entire state-action history if we ever reach the gift\n",
    "        if move>0:\n",
    "            if len(state_history) < steps_looking_into_future:\n",
    "                state_history.append(observation)\n",
    "            else:\n",
    "                state_history.pop(0)\n",
    "                state_history.append(observation)\n",
    "\n",
    "        observation = new_observation\n",
    "\n",
    "        # Execute action\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Learn\n",
    "        if (not terminated):\n",
    "            if new_observation == observation:\n",
    "                reward = -1\n",
    "                agent.force_learn(observation, action, reward)\n",
    "            else:\n",
    "                agent.learn(observation, action, reward)\n",
    "\n",
    "        # Stop if dead\n",
    "        if terminated:\n",
    "            # if the player did not collect the gift and the game stops it will reach this point with reward=0\n",
    "            if reward == 0 and move != max_moves:\n",
    "                reward = -2 # punishment\n",
    "            # If the player collected the gift it will reach this point with reward=1\n",
    "            elif reward == 1:\n",
    "                reward = steps_looking_into_future+1\n",
    "                # reward the last x moves it took to get to the finish.\n",
    "                print('Reached the Gift')\n",
    "                for x, _ in enumerate(range(steps_looking_into_future)):\n",
    "                    new_reward = reward - (reward-x-1)\n",
    "                    try:\n",
    "                        agent.learn(state_history[x], action_history[x], new_reward)\n",
    "                    except:\n",
    "                        raise Exception('Tried to look more steps into the future than the steps it took to reach the finish. Try lowering the variable `steps_looking_into_future`')\n",
    "            # force learn the last action (this usually is something really good or really bad)\n",
    "            agent.force_learn(observation, action, reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Agent\n",
    "For testing the agent we will make the map visible so we can see how the agent behaves. To make the map visible we have to redifine the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', \n",
    "               render_mode='human',\n",
    "               is_slippery=False, \n",
    "               map_name='8x8', \n",
    "               desc=desc_base)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached the Gift\n",
      "Reached the Gift\n",
      "Reached the Gift\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "max_moves = 1000 # max moves before resetting env\n",
    "episodes = 3 # amouint of tries\n",
    "exploration_rate=0.0 # the chance of ignoring what we've learned and doing a random action\n",
    "steps_looking_into_future = 36 # the amount of steps we should look into the future\n",
    "agent = agent11 # set agent\n",
    "\n",
    "for episode in range(episodes):\n",
    "    new_observation = 0  # We consider that we always start top left // if you wanna start somewhere else you will have to change this variable \n",
    "    action_history = [] # clear the action history\n",
    "    state_history = [] # clear the state history\n",
    "    env.reset()\n",
    "    for move in range(max_moves):  \n",
    "\n",
    "        # keep track of the action history\n",
    "        # later used to laern the entire state-action history if we ever reach the gift\n",
    "        if move>0:\n",
    "            if len(action_history) < steps_looking_into_future:\n",
    "                action_history.append(action)\n",
    "            else: \n",
    "                action_history.pop(0)\n",
    "                action_history.append(action)\n",
    "\n",
    "        # Get an action based of the Q-table\n",
    "        action = agent.step(new_observation, exploration_rate)\n",
    "\n",
    "        # keep track of the state history\n",
    "        # later used to laern the entire state-action history if we ever reach the gift\n",
    "        if move>0:\n",
    "            if len(state_history) < steps_looking_into_future:\n",
    "                state_history.append(observation)\n",
    "            else:\n",
    "                state_history.pop(0)\n",
    "                state_history.append(observation)\n",
    "\n",
    "        observation = new_observation\n",
    "\n",
    "        # Execute action\n",
    "        new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Learn\n",
    "        if (not terminated):\n",
    "            if new_observation == observation:\n",
    "                reward = -1\n",
    "                agent.force_learn(observation, action, reward)\n",
    "            else:\n",
    "                agent.learn(observation, action, reward)\n",
    "\n",
    "        # Stop if dead\n",
    "        if terminated:\n",
    "            # if the player did not collect the gift and the game stops it will reach this point with reward=0\n",
    "            if reward == 0 and move != max_moves:\n",
    "                reward = -2 # punishment\n",
    "            # If the player collected the gift it will reach this point with reward=1\n",
    "            elif reward == 1:\n",
    "                reward = steps_looking_into_future+1\n",
    "                # reward the last x moves it took to get to the finish.\n",
    "                print('Reached the Gift')\n",
    "                for x, _ in enumerate(range(steps_looking_into_future)):\n",
    "                    new_reward = reward - (reward-x-1)\n",
    "                    try:\n",
    "                        agent.learn(state_history[x], action_history[x], new_reward)\n",
    "                    except:\n",
    "                        raise Exception('Tried to look more steps into the future than the steps it took to reach the finish. Try lowering the variable `steps_looking_into_future`')\n",
    "            # force learn the last action (this usually is something really good or really bad)\n",
    "            agent.force_learn(observation, action, reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We will take a look into my Q-Table now. Here is the definition for each number you will see in the Q-Table.\n",
    "\n",
    "- -2: will result in death\n",
    "- -1: will result in standing still\n",
    "- 0: just walk\n",
    "- 1: will result in being 15 steps away from gift\n",
    "- 2: will result in being 14 step away from gift\n",
    "- 3: will result in being 13 steps away from gift\n",
    "- 4: will result in being 12 steps away from gift\n",
    "- 5: will result in being 11 step away from gift\n",
    "- 6: will result in being 10 steps away from gift\n",
    "- 7: will result in being 9 step away from gift\n",
    "- 8: will result in being 8 step away from gift\n",
    "- 9: will result in being 7 step away from gift\n",
    "- 10: will result in being 6 step away from gift\n",
    "- 11: will result in being 5 steps away from gift\n",
    "- 12: will result in being 4 step away from gift\n",
    "- 13: will result in being 3 step away from gift\n",
    "- 14: will result in being 2 step away from gift\n",
    "- 15: will result in being 1 step away from gift\n",
    "- 16: will result in getting gift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0   1  2  3\n",
      "0   -1   0  1 -1\n",
      "1    0  -2  2 -1\n",
      "2    0   2  4 -1\n",
      "3    0  -2  5 -1\n",
      "4    0   0  6 -1\n",
      "5    0  -2  7 -1\n",
      "6    2   8  4 -1\n",
      "7    5  -2 -1 -1\n",
      "8   -1   0 -2  0\n",
      "9    0   0  0  0\n",
      "10  -2   0 -2  3\n",
      "11   0   0  0  0\n",
      "12  -2   0 -2  0\n",
      "13   0   0  0  0\n",
      "14  -2   9 -2  1\n",
      "15   0   0  0  0\n",
      "16  -1   0 -2  0\n",
      "17   0   0  0  0\n",
      "18  -2   0 -2  0\n",
      "19   0   0  0  0\n",
      "20  -2  -2  0  0\n",
      "21   0  -2  0 -2\n",
      "22   0  10  8  0\n",
      "23   9  -2 -1 -2\n",
      "24  -1   0 -2  0\n",
      "25   0   0  0  0\n",
      "26  -2   0 -2  0\n",
      "27   0   0  0  0\n",
      "28   0   0  0  0\n",
      "29   0   0  0  0\n",
      "30  -2  11 -2  0\n",
      "31   0   0  0  0\n",
      "32  -1   0  0  0\n",
      "33   0  -2  0 -2\n",
      "34   0  -2 -2  0\n",
      "35   0   0  0  0\n",
      "36   0   0  0  0\n",
      "37  12   0  0  0\n",
      "38  12  -2  0  0\n",
      "39   0   0  0  0\n",
      "40  -1   0 -2  0\n",
      "41   0   0  0  0\n",
      "42   0   0  0  0\n",
      "43   0   0  0  0\n",
      "44   0   0 -2  0\n",
      "45   0   0  0  0\n",
      "46   0   0  0  0\n",
      "47   0   0  0  0\n",
      "48  -1   0 -2  0\n",
      "49   0   0  0  0\n",
      "50  -2   0  0 -2\n",
      "51   0  -2  0  0\n",
      "52   0   0  0  0\n",
      "53   0   0  0  0\n",
      "54   0   0  0  0\n",
      "55   0   0  0  0\n",
      "56  -1  -1  0  0\n",
      "57   0  -1  0 -2\n",
      "58   0  -1 -2  0\n",
      "59   0   0  0  0\n",
      "60   0   0  0  0\n",
      "61   0   0  0  0\n",
      "62   0   0  0  0\n",
      "63   0   0  0  0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print((agent11.evaluate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "- https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
    "- https://www.youtube.com/watch?v=ZhoIgo3qqLU&t=277s\n",
    "- https://www.youtube.com/watch?v=V65phXUGb4I\n",
    "- https://towardsdatascience.com/q-learning-for-beginners-2837b777741\n",
    "- https://www.youtube.com/watch?v=Vrro7W7iW2w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
