{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: keras-rl2\n",
      "Version: 1.0.5\n",
      "Summary: Deep Reinforcement Learning for Tensorflow 2 Keras\n",
      "Home-page: https://github.com/wau/keras-rl2\n",
      "Author: Taylor McNally\n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\users\\kaan-\\anaconda3\\envs\\gym37\\lib\\site-packages\n",
      "Requires: tensorflow\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kaan-\\anaconda3\\envs\\gym37\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Make sure to use Python: 3.7.16\n",
    "!pip install tensorflow==2.10.0\n",
    "!pip show keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.18 \n",
      "NumPy: 1.24.4\n",
      "gym: 0.26.2\n",
      "TensorFlow: 2.13.0\n",
      "Keras-rl2: 1.0.5\n"
     ]
    }
   ],
   "source": [
    "# env: gym37\n",
    "import sys # 3.7.16\n",
    "import random\n",
    "import numpy as np # 1.21.6\n",
    "import gym # 0.25.2\n",
    "import tensorflow # 2.10.0\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam    \n",
    "import rl # keras-rl2==1.0.5\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "print(\"Python: \" + sys.version[0:7])\n",
    "print(\"NumPy: \" + np.__version__)\n",
    "print(\"gym: \" + gym.__version__)\n",
    "print(\"TensorFlow: \" + tensorflow.__version__)\n",
    "print(\"Keras-rl2: 1.0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states (1, 4)\n",
    "(1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1, states)))\n",
    "model.add(Dense(24, activation=\"relu\"))\n",
    "model.add(Dense(24, activation=\"relu\"))\n",
    "model.add(Dense(actions, activation=\"linear\"))\n",
    "\n",
    "agent = DQNAgent(\n",
    "    model=model,\n",
    "    memory=SequentialMemory(limit=50000, window_length=1),\n",
    "    policy=BoltzmannQPolicy(),\n",
    "    nb_actions=actions,\n",
    "    nb_steps_warmup=10,\n",
    "    target_model_update=0.01\n",
    ")\n",
    "\n",
    "agent.compile(optimizer=Adam(learning_rate=0.001), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 185s 18ms/step - reward: 1.0000\n",
      "53 episodes - episode_reward: 187.094 [150.000, 248.000] - loss: 3.043 - mae: 27.078 - mean_q: 54.968\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 198s 20ms/step - reward: 1.0000\n",
      "49 episodes - episode_reward: 203.388 [175.000, 261.000] - loss: 3.169 - mae: 39.208 - mean_q: 79.212\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 1.0000\n",
      "48 episodes - episode_reward: 208.500 [175.000, 238.000] - loss: 2.373 - mae: 39.637 - mean_q: 79.876\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 158s 16ms/step - reward: 1.0000\n",
      "43 episodes - episode_reward: 233.814 [166.000, 500.000] - loss: 1.528 - mae: 37.609 - mean_q: 75.681\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 162s 16ms/step - reward: 1.0000\n",
      "25 episodes - episode_reward: 397.520 [173.000, 500.000] - loss: 7.048 - mae: 42.817 - mean_q: 86.124\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 237s 24ms/step - reward: 1.0000\n",
      "52 episodes - episode_reward: 193.385 [9.000, 330.000] - loss: 22.863 - mae: 60.690 - mean_q: 121.771\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 1.0000\n",
      "39 episodes - episode_reward: 256.103 [194.000, 500.000] - loss: 8.538 - mae: 62.424 - mean_q: 125.307\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      " 2939/10000 [=======>......................] - ETA: 1:48 - reward: 1.0000done, took 1430.322 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20c35b53648>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.fit(env=env, nb_steps=100000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 500.000, steps: 500\n",
      "Episode 6: reward: 500.000, steps: 500\n",
      "Episode 7: reward: 500.000, steps: 500\n",
      "Episode 8: reward: 500.000, steps: 500\n",
      "Episode 9: reward: 500.000, steps: 500\n",
      "Episode 10: reward: 500.000, steps: 500\n",
      "500.0\n"
     ]
    }
   ],
   "source": [
    "results = agent.test(env, nb_episodes=10, verbose=1)\n",
    "print(np.mean(results.history[\"episode_reward\"]))\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
