{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpAGnrsKK_NJ"
   },
   "source": [
    "## Under Construction!\n",
    "\n",
    "The **DQNAgent** class comprises methods for updating the target model, constructing neural networks for action and target networks, executing forward passes, recording experiences, decision-making based on the Boltzmann policy, computing TD errors for prioritization during replay, and implementing experience replay mechanisms for training.\n",
    "\n",
    "\n",
    "This code is a PyTorch implementation of a *Double Deep Q-Network (DDQN) agent with prioritized experience replay* for the CartPole-v1 environment provided by [Frama Foundation Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "\n",
    "### Recent Updates:\n",
    "\n",
    "- **Algorithm Updates:**\n",
    "  - Replaced Q-learning with expected-sarsa, specifically utilizing averageQ over maxQ. Expected Sarsa is on-policy learning, which is known to work better with function approximation (the RL deadly triad)\n",
    "  - Transitioned from epsilon-greedy to a Boltzmann policy for action selection. Why: I noticed that random guessing works better than a poorly trained network. This is because a slightest bias in the poorly trained network will make the agent to push the cart in one direction, causing it to fail. Random guessing however is on average unbiased. Thus, I choose the Boltzmann policy that is always probabilistic and lower the temperature slowly.\n",
    "\n",
    "- **State Utilization:**\n",
    "  - Introduced an additional output utilizing states in an attempt to enhance training stability. Initial observations suggest limited effectiveness, prompting consideration for incorporating state learning into the summary writer for further analysis (to be done). CUrrently states are an output of the system, the idea was that since Q values constantly change and we know bootstrapping is unstable, I tried to give the network a stable output. To this point not effective on its own.\n",
    "\n",
    "- **Optimization Technique:**\n",
    "  - Discontinued the active use of Optuna for hyperparameter optimization. *Note: Optuna was previously used for hyperparameter optimization.*\n",
    "\n",
    "- **Visualization and Hyperparameter Tuning:**\n",
    "  - Utilizes TensorBoard for visualization, providing insights into training metrics and Q-value graphs.\n",
    "  - Previously, Optuna was used for hyperparameter optimization; however, the code no longer actively employs it.\n",
    "\n",
    "Remember to monitor the TensorBoard Q graphs for a visual representation of the training progress.\n",
    "\n",
    "Author: Iman Mossavat  \n",
    "Date: 19-December-2023  \n",
    "Institution: Fontys ICT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFNhW_AoVskN",
    "outputId": "c5a6a83d-6467-48a1-989b-0fcf757149e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium[classic-control]\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (4.5.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium[classic-control])\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.5.2)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install gymnasium[classic-control]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:32:27.614139400Z",
     "start_time": "2023-12-19T13:32:17.485070200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_keYIC-LSGdB",
    "outputId": "80042729-6e71-43da-fefb-9325c31bd4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "     ------------------------------------ 413.4/413.4 kB 737.1 kB/s eta 0:00:00\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
      "     ------------------------------------ 230.6/230.6 kB 941.6 kB/s eta 0:00:00\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\school\\semester 6 ai\\venv\\lib\\site-packages (from optuna) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\school\\semester 6 ai\\venv\\lib\\site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\school\\semester 6 ai\\venv\\lib\\site-packages (from optuna) (2.0.23)\n",
      "Requirement already satisfied: tqdm in c:\\school\\semester 6 ai\\venv\\lib\\site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in c:\\school\\semester 6 ai\\venv\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.6/78.6 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\school\\semester 6 ai\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\school\\semester 6 ai\\venv\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\school\\semester 6 ai\\venv\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\school\\semester 6 ai\\venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.0 alembic-1.13.0 colorlog-6.8.0 optuna-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy\n",
    "!pip install optuna\n",
    "#!pip install matplotlib\n",
    "#!pip install torch torchvision\n",
    "#!pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:33:20.962521900Z",
     "start_time": "2023-12-19T13:33:19.144024800Z"
    },
    "id": "P4ib_MKpSpq1"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "# Mount your Google Drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import optuna\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "log_dir = \"runs\"\n",
    "empty_initial_frames = True\n",
    "FRAMES = True # create an environment with RGB frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:33:22.776549400Z",
     "start_time": "2023-12-19T13:33:22.578056600Z"
    },
    "id": "liIen25HVyc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(gym.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:33:31.407548200Z",
     "start_time": "2023-12-19T13:33:23.633934500Z"
    },
    "id": "CCoJXFb5UZvM"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:33:34.177070800Z",
     "start_time": "2023-12-19T13:33:34.162073200Z"
    },
    "id": "UvpDgmtzgyec"
   },
   "outputs": [],
   "source": [
    "class FrameHandler:\n",
    "    def __init__(self):\n",
    "        self.frame_stack = deque(maxlen=4) # stack of 4 pytorch tensors containing containing preprocessed frames\n",
    "\n",
    "    def preprocess_frame(self, frame, D=84):\n",
    "        \"\"\"Resizing to DxD (84x84) pixels numpy array, converting to grayscale, and normalizing pixel values. \"\"\"\n",
    "\n",
    "        frame = cv2.resize(frame, (D, D))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame = frame / 255.0\n",
    "        return frame\n",
    "\n",
    "    def update_framestack(self, frame):\n",
    "        # Pre-process, converts to tensor and append the new frame at the beginning\n",
    "        preprocessed_frame = self.preprocess_frame(frame)\n",
    "        preprocessed_frame_tensor = self.convert_frame_to_tensor(preprocessed_frame)\n",
    "\n",
    "        self.frame_stack.appendleft(preprocessed_frame_tensor)\n",
    "\n",
    "    def initialize_frame_stack(self, frame, empty_initial_frames=True):\n",
    "        self.frame_stack.clear()\n",
    "        empty_frame = np.zeros_like(frame)\n",
    "\n",
    "\n",
    "        for _ in range(4):\n",
    "          if empty_initial_frames:\n",
    "            self.update_framestack(empty_frame)\n",
    "          else:\n",
    "            self.frame_stack.extend(frame)\n",
    "\n",
    "        self.update_framestack(frame)\n",
    "\n",
    "    def convert_frame_to_tensor(self, frame):\n",
    "        frame_tensor = torch.from_numpy(frame).float()\n",
    "        return frame_tensor\n",
    "\n",
    "    def clear(self):\n",
    "       self.frame_stack.clear()\n",
    "\n",
    "    def convert_frame_stack_to_tensor(self):\n",
    "      # useful to build an input to CNN\n",
    "      frames_tensor = torch.stack(list(self.frame_stack))  # Stack the frames along a new dimension\n",
    "      frames_tensor = frames_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "      return frames_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcqsZSKBzGTz",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the CNN layers for processing the stack of frames\n",
    "self.conv_layers = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=4, out_channels=8, kernel_size=5, stride=1),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # Adding MaxPooling\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, groups = 2),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # Adding MaxPooling\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, groups = 2),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # Adding MaxPooling\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # Adding MaxPooling\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1),\n",
    "    nn.Flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:33:36.008348Z",
     "start_time": "2023-12-19T13:33:35.987722800Z"
    },
    "id": "1zGgv6ZE1_vv"
   },
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, state_size= 4, image_size=(4, 84, 84), action_size=2, dropout_prob= 0.3):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Define the CNN layers for processing the stack of frames\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Adding MaxPooling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_prob),  # Adding dropout after ReLU\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, groups = 8),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Adding MaxPooling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_prob),  # Adding dropout after ReLU\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, groups = 4),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Adding MaxPooling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_prob),  # Adding dropout after ReLU\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, groups = 2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Adding MaxPooling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_prob),  # Adding dropout after ReLU\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_prob),  # Adding dropout after ReLU\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Calculate the input size for the fully connected layers after CNN\n",
    "        conv_output_size = self._get_conv_output_size(self.image_size)\n",
    "\n",
    "\n",
    "        # Define the fully connected layers for Q-values and state replication\n",
    "        self.fc_q_values = nn.Sequential(\n",
    "            nn.Linear(128, self.action_size)\n",
    "        )\n",
    "\n",
    "        self.fc_replicate_state = nn.Sequential(\n",
    "            nn.Linear(128, state_size)  # Adjust output size for state replication\n",
    "        )\n",
    "\n",
    "        self.fc_common = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, 128),  # Adjust output size for state replication\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _get_conv_output_size(self, shape):\n",
    "        dummy_input = torch.rand(1, *shape)\n",
    "        dummy_output = self.conv_layers(dummy_input)\n",
    "        return dummy_output.size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_common(x)\n",
    "        replicated_state = self.fc_replicate_state(x)\n",
    "        q_values = self.fc_q_values(x)\n",
    "        return q_values, replicated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:33:37.033118300Z",
     "start_time": "2023-12-19T13:33:36.987059200Z"
    },
    "id": "5htv9_Yol05s"
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, maxlen):\n",
    "        \"\"\"\n",
    "        Initializes a Memory object with a deque.\n",
    "        This object allows for management of temporal difference error (TDE) and prioritiezed experience replay.\n",
    "\n",
    "        Args:\n",
    "            maxlen (int): Maximum length of the memory.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=maxlen)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the deque.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the memory deque.\n",
    "        \"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "    def remember(self, observation, action, reward, next_observation, done, tde, auxiliary_data=None):\n",
    "        \"\"\"\n",
    "      remembers an experience in the agent's memory.\n",
    "\n",
    "      Args:\n",
    "          observation (object): The current observation/state.\n",
    "          action (int): The action taken in the current state.\n",
    "          reward (float): The reward received after taking the action.\n",
    "          next_observation (object): The next observation/state after taking the action.\n",
    "          done (bool): A flag indicating if the episode terminates after this step.\n",
    "          auxiliary_data (object, optional): Additional data associated with the experience. Defaults to None.\n",
    "\n",
    "      Returns:\n",
    "          float: The Temporal Difference Error (tde) computed from the given experience.\n",
    "        \"\"\"\n",
    "        if auxiliary_data is not None:\n",
    "            if not isinstance(auxiliary_data, torch.Tensor):\n",
    "                auxiliary_data_torch = torch.from_numpy(auxiliary_data).float().unsqueeze(0)\n",
    "            else:\n",
    "                auxiliary_data_torch = auxiliary_data\n",
    "            self.memory.append((observation, action, reward, next_observation, done, auxiliary_data_torch, tde))\n",
    "        else:\n",
    "            self.memory.append((observation, action, reward, next_observation, done, None, tde))\n",
    "\n",
    "    def sample_batch_with_priority(self, batch_size, beta):\n",
    "        \"\"\"\n",
    "        Samples a batch with priority based on the calculated probabilities.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Size of the batch to be sampled.\n",
    "            beta (float): Parameter used to calculate probabilities.\n",
    "\n",
    "        Returns:\n",
    "            list: A batch of experiences sampled with priority.\n",
    "        \"\"\"\n",
    "        tde = self.get_all_tde()\n",
    "        prob = self.calculate_probabilities(tde, beta)\n",
    "        indices = np.random.choice(len(self.memory), size=batch_size, p=prob, replace=False)\n",
    "        minibatch = [self.memory[i] for i in indices]\n",
    "        return minibatch, indices\n",
    "\n",
    "\n",
    "    def get_all_tde(self):\n",
    "        tde = abs(np.array([item[-1] for item in self.memory]))\n",
    "        return tde\n",
    "\n",
    "    def sort_memory_by_tde(self):\n",
    "\n",
    "      tde_values = np.array([item[-1] for item in self.memory])\n",
    "      indices = np.argsort(tde_values)\n",
    "      self.memory = deque([self.memory[i] for i in indices], maxlen=self.memory.maxlen)\n",
    "\n",
    "    def calculate_probabilities(self, tde, beta):\n",
    "        exp_tde = np.exp(beta * (tde - np.median(tde)))\n",
    "        prob = exp_tde / np.sum(exp_tde)\n",
    "        return prob\n",
    "\n",
    "    def update_td_errors_by_index(self, indices, targets, originals):\n",
    "        for i,index in enumerate(indices):\n",
    "            _, action, _, _, _, _, _ = self.memory[index]  # Retrieve action from memory\n",
    "\n",
    "            td_error = targets[i][action].detach().numpy() - originals[i][action].detach().numpy()\n",
    "\n",
    "            updated_entry = list(self.memory[index])\n",
    "            updated_entry[-1] = td_error\n",
    "            self.memory[index] = tuple(updated_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPcp4Ws9YICu"
   },
   "source": [
    "We have deque that serves as the replay memory, where each element in the replay memory is a tuple consisting of state, action, next state, reward, and TDE. Additionally, each state in these tuples is represented as a deque object that holds a stack of four images. This structure is used to store and manage experiences for training your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:33:38.930538600Z",
     "start_time": "2023-12-19T13:33:38.888895700Z"
    },
    "id": "SshpTNF_STlD"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, image_size, action_size, state_size = 4, maxlen=2048, minlen = 1024,epsilon = 1.0,epsilon_min = 0.01,epsilon_decay = 0.99,gamma = 0.90, update_frequency= 100):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.image_size = image_size # (4, 84, 84) for a image\n",
    "        self.state_size = state_size #  (1,4) for gym states (cart-pole)\n",
    "        self.action_size = action_size\n",
    "        self.maxlen = maxlen\n",
    "        self.memory = Memory(maxlen=self.maxlen)\n",
    "        self.minlen= minlen\n",
    "\n",
    "        self.frame_handler = FrameHandler()\n",
    "\n",
    "        self.gamma = gamma  # Discount factor\n",
    "\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        if self.model.dropout_prob == 0:\n",
    "          self.DropOut = False\n",
    "        else:\n",
    "          self.DropOut = True\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        self.target_model.eval()\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_model_(self):\n",
    "      with torch.no_grad():\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        input_data = torch.rand(1, *self.image_size)\n",
    "        assert torch.allclose(self.model(input_data), self.target_model(input_data), atol=1e-6)\n",
    "        self.target_model.eval()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        with torch.no_grad():\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "            if not self.DropOut:\n",
    "              input_data = torch.rand(1, *self.image_size)\n",
    "              model_outputs = self.model(input_data)\n",
    "              target_model_outputs = self.target_model(input_data)\n",
    "\n",
    "              # Separate the outputs if they're tuples\n",
    "              model_output_1, model_output_2 = model_outputs\n",
    "              target_model_output_1, target_model_output_2 = target_model_outputs\n",
    "\n",
    "\n",
    "              # Assert for the first output\n",
    "              assert torch.allclose(model_output_1, target_model_output_1, atol=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "              # Assert for the second output\n",
    "              assert torch.allclose(model_output_2, target_model_output_2, atol=1e-6)\n",
    "\n",
    "            self.target_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "      return CustomCNN(image_size= self.image_size, action_size= self.action_size)\n",
    "\n",
    "    def forward(self, x, mode='primary'):\n",
    "        # accepts numpy array or PyTorch, can choose with DQN policy to use\n",
    "\n",
    "        if mode == 'primary':\n",
    "            model = self.model\n",
    "        elif mode == 'target':\n",
    "            model = self.target_model\n",
    "        else:\n",
    "            raise ValueError('unidentified forward calc mode')\n",
    "\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x).float().unsqueeze(0)\n",
    "        elif not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a Numpy array or a PyTorch tensor\")\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        # Handle tuple output, if any, by unpacking and squeezing\n",
    "        if isinstance(output, tuple):\n",
    "            return tuple(o.squeeze() for o in output)\n",
    "        else:\n",
    "            return output.squeeze()\n",
    "\n",
    "\n",
    "    def act(self, observation):\n",
    "        output = self.forward(observation, mode='primary')\n",
    "        if isinstance(output, tuple):\n",
    "            q_values = output[0]\n",
    "        else:\n",
    "            q_values = output\n",
    "\n",
    "        # Calculate action probabilities using softmax with temperature (epsilon)\n",
    "        action_probs = F.softmax(q_values / self.epsilon, dim=-1)\n",
    "\n",
    "        # Sample an action according to the probability distribution\n",
    "        action = np.random.choice(self.action_size, p=action_probs.detach().numpy())\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def td_error(self, observation, action, reward, next_observation, done):\n",
    "      # Get Q-values for the current and next observations from the policy network\n",
    "      q_values_observation,_ = self.forward(observation)\n",
    "      q_values_next_observation,_ = self.forward(next_observation)\n",
    "\n",
    "      # Calculate the target Q-value using the Q-learning update rule\n",
    "      target = q_values_observation.clone().detach()  # Detach from computational graph\n",
    "\n",
    "      if not done:\n",
    "          max_next_q_value = torch.max(q_values_next_observation) # small bug: this TDE is for Q-learning not expected sarsa, but I guess for initialization that it is used, it does not matter too much.\n",
    "          tde = q_values_observation[action] - (reward + self.gamma * max_next_q_value)\n",
    "      else:\n",
    "          tde = q_values_observation[action] - reward\n",
    "      tde = tde.detach().item()\n",
    "\n",
    "      assert isinstance(tde, (np.generic, float)) and np.isscalar(tde), f\"tde should be a NumPy scalar tde: {tde}, act {action} reward {reward} ntype: {type(tde)}\"\n",
    "\n",
    "      return tde\n",
    "\n",
    "\n",
    "    def refresh_memory(self):\n",
    "        temp_storage = self.memory.memory.copy()\n",
    "        self.memory.memory.clear()\n",
    "        for observation, action, reward, next_observation, done, auxiliary_data, _ in temp_storage:\n",
    "            tde = self.td_error(observation, action, reward, next_observation, done)\n",
    "            self.memory.remember(\n",
    "                          observation=observation,\n",
    "                          action=action,\n",
    "                          reward=reward,\n",
    "                          next_observation=next_observation,\n",
    "                          done=done,\n",
    "                          tde=tde,\n",
    "                          auxiliary_data=auxiliary_data\n",
    "                      )\n",
    "        self.memory.sort_memory_by_tde()\n",
    "        print('Memory refreshed / sorted')\n",
    "\n",
    "    def replay(self, batch_size, optimizer, beta=1):\n",
    "        assert self.minlen > batch_size\n",
    "        if len(self.memory) < self.minlen:\n",
    "            return None, None\n",
    "\n",
    "        minibatch, indices = self.memory.sample_batch_with_priority(batch_size=batch_size, beta=beta)\n",
    "\n",
    "        targets = torch.zeros(batch_size, self.action_size)\n",
    "        originals = torch.zeros(batch_size, self.action_size)\n",
    "        aux_outputs = torch.zeros(batch_size, self.state_size)  #\n",
    "        auxiliary_data_ = torch.zeros(batch_size,  self.state_size)  #\n",
    "\n",
    "        for i, (observation, action, reward, next_observation, done, auxiliary_data,tde) in enumerate(minibatch):\n",
    "            # Forward pass to get Q-values and auxiliary output if available\n",
    "            if auxiliary_data is not None:\n",
    "                q_values_observation, aux_output = self.forward(observation, mode='primary')\n",
    "                q_values_next_observation, _ = self.forward(next_observation, mode='target')\n",
    "            else:\n",
    "                q_values_observation = self.forward(observation, mode='primary')\n",
    "                q_values_next_observation = self.forward(next_observation, mode='target')\n",
    "\n",
    "            assert q_values_observation is not None\n",
    "            assert q_values_next_observation is not None\n",
    "            originals[i] = q_values_observation.clone()  # Store original Q-values\n",
    "            targets[i] = q_values_observation.clone().detach()\n",
    "\n",
    "            if auxiliary_data is not None:\n",
    "                # Use the auxiliary output for the second output in the network\n",
    "                # Modify this section to suit how the auxiliary output affects target calculations\n",
    "                aux_outputs[i] = aux_output\n",
    "                auxiliary_data_[i] = auxiliary_data\n",
    "\n",
    "            if not done:\n",
    "\n",
    "                if True: #boltzmann on-policy sarsa\n",
    "                  # Calculate action probabilities using Boltzmann distribution\n",
    "                  action_probs = F.softmax(q_values_next_observation / self.epsilon, dim=-1)\n",
    "\n",
    "                  # Calculate the average Q-value based on the Boltzmann policy\n",
    "                  avg_next_q_value = torch.sum(action_probs * q_values_next_observation)\n",
    "\n",
    "                  # Update the target based on the average Q-value\n",
    "                  targets[i][action] = reward + self.gamma * avg_next_q_value\n",
    "\n",
    "            else:\n",
    "                targets[i][action] = reward\n",
    "\n",
    "        loss_q = nn.MSELoss()(originals, targets)\n",
    "\n",
    "        if auxiliary_data is not None:\n",
    "            # Calculate auxiliary loss if auxiliary_data is available\n",
    "            # Modify this section according to how the auxiliary output influences the loss calculation\n",
    "            auxiliary_loss = nn.MSELoss()(aux_output, auxiliary_data)\n",
    "            loss = loss_q + auxiliary_loss\n",
    "        else:\n",
    "            # Use only the Q-value loss if there's no auxiliary data\n",
    "            loss = loss_q\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if self.total_steps % self.update_frequency == 0:\n",
    "          self.update_target_model()  # Update the target model\n",
    "          print('update target')\n",
    "          self.refresh_memory()\n",
    "          print('memory refresh')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.total_steps += 1\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        if False:  # keep cycling from hot to cold (these if True statements need to be coded properly with a flag to turn on or off later)\n",
    "            if self.epsilon < self.epsilon_min or self.epsilon == self.epsilon_min:\n",
    "                self.epsilon = 1\n",
    "\n",
    "\n",
    "        # Update TD errors for elements used in the minibatch\n",
    "        self.memory.update_td_errors_by_index(indices, targets, originals)\n",
    "\n",
    "        return loss.item(), originals\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n2Drd7nMhs1"
   },
   "source": [
    "Custom reward (not used/tested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxafvkRfMdgX"
   },
   "source": [
    "Hyper pararmeter optimization via Optuna. Currently inactive, if you want to use this say for  `epsilon_min`, use\n",
    "\n",
    "    epsilon_min = trial.suggest_float('epsilon_min', 1e-3, 1e-1, log=True)\n",
    "\n",
    "and change `n_trials` in to set the number of search iterations\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, writer), n_trials=1)  # Adjust the number of trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:40:57.840294300Z",
     "start_time": "2023-12-19T13:33:41.602576500Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "5GjEMOeU0sMA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 14:33:41,611] A new study created in memory with name: no-name-1609780c-835e-4fe0-95e5-39e4cfd34eff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/600, survival time: 31, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 2/600, survival time: 10, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 3/600, survival time: 26, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 4/600, survival time: 12, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 5/600, survival time: 16, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 6/600, survival time: 11, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 7/600, survival time: 15, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 8/600, survival time: 29, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 9/600, survival time: 12, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 10/600, survival time: 32, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 11/600, survival time: 10, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 12/600, survival time: 27, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 13/600, survival time: 16, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 14/600, survival time: 33, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 15/600, survival time: 14, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 16/600, survival time: 21, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 17/600, survival time: 13, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 18/600, survival time: 15, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 19/600, survival time: 10, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 20/600, survival time: 22, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 21/600, survival time: 13, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 22/600, survival time: 27, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 23/600, survival time: 16, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 24/600, survival time: 28, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 25/600, survival time: 25, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 26/600, survival time: 19, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 27/600, survival time: 45, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 28/600, survival time: 43, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 29/600, survival time: 22, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 30/600, survival time: 33, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 31/600, survival time: 17, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 32/600, survival time: 18, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 33/600, survival time: 31, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 34/600, survival time: 22, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 35/600, survival time: 17, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 36/600, survival time: 23, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 37/600, survival time: 15, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 38/600, survival time: 53, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 39/600, survival time: 33, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 40/600, survival time: 41, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 41/600, survival time: 24, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 42/600, survival time: 10, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 43/600, survival time: 27, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 44/600, survival time: 17, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 45/600, survival time: 16, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 46/600, survival time: 12, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 47/600, survival time: 34, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 48/600, survival time: 23, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 49/600, survival time: 31, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 50/600, survival time: 20, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 51/600, survival time: 16, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 52/600, survival time: 16, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 53/600, survival time: 17, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 54/600, survival time: 71, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 55/600, survival time: 39, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 56/600, survival time: 27, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 57/600, survival time: 8, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 58/600, survival time: 18, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 59/600, survival time: 22, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 60/600, survival time: 20, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 61/600, survival time: 9, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 62/600, survival time: 15, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 63/600, survival time: 16, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 64/600, survival time: 13, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 65/600, survival time: 13, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 66/600, survival time: 19, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 67/600, survival time: 17, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 68/600, survival time: 20, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 69/600, survival time: 44, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 70/600, survival time: 14, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 71/600, survival time: 18, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 72/600, survival time: 11, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 73/600, survival time: 60, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 74/600, survival time: 25, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 75/600, survival time: 45, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 76/600, survival time: 14, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 77/600, survival time: 12, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 78/600, survival time: 33, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 79/600, survival time: 27, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 80/600, survival time: 32, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 81/600, survival time: 12, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 82/600, survival time: 10, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 83/600, survival time: 17, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 84/600, survival time: 18, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 85/600, survival time: 14, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 86/600, survival time: 37, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 87/600, survival time: 57, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 88/600, survival time: 22, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 89/600, survival time: 17, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 90/600, survival time: 13, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 91/600, survival time: 11, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 92/600, survival time: 18, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 93/600, survival time: 15, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 94/600, survival time: 29, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 95/600, survival time: 30, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 96/600, survival time: 35, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 97/600, survival time: 12, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 98/600, survival time: 16, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 99/600, survival time: 30, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 100/600, survival time: 17, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 101/600, survival time: 18, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 102/600, survival time: 14, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 103/600, survival time: 18, Average Loss: inf, epsilon: 20.00000\n",
      "Episode: 104/600, survival time: 19, Average Loss: inf, epsilon: 20.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update target\n",
      "Memory refreshed / sorted\n",
      "memory refresh\n",
      "Episode: 105/600, survival time: 25, Average Loss: 36.4675, epsilon: 19.73121\n",
      "Episode: 106/600, survival time: 18, Average Loss: 21.5561, epsilon: 18.11081\n",
      "Episode: 107/600, survival time: 20, Average Loss: 28.3127, epsilon: 16.47420\n",
      "Episode: 108/600, survival time: 18, Average Loss: 15.3696, epsilon: 15.12127\n",
      "Episode: 109/600, survival time: 15, Average Loss: 21.2289, epsilon: 14.06853\n",
      "Episode: 110/600, survival time: 21, Average Loss: 13.6062, epsilon: 12.73962\n",
      "update target\n",
      "Memory refreshed / sorted\n",
      "memory refresh\n",
      "Episode: 111/600, survival time: 36, Average Loss: 13.9540, epsilon: 10.78160\n",
      "Episode: 112/600, survival time: 11, Average Loss: 14.6749, epsilon: 10.21359\n",
      "Episode: 113/600, survival time: 8, Average Loss: 10.7686, epsilon: 9.80731\n",
      "Episode: 114/600, survival time: 14, Average Loss: 9.3312, epsilon: 9.16577\n",
      "Episode: 115/600, survival time: 24, Average Loss: 11.1247, epsilon: 8.18842\n",
      "update target\n",
      "Memory refreshed / sorted\n",
      "memory refresh\n",
      "Episode: 116/600, survival time: 16, Average Loss: 9.9990, epsilon: 7.58406\n",
      "Episode: 117/600, survival time: 38, Average Loss: 7.5132, epsilon: 6.36079\n",
      "Episode: 118/600, survival time: 20, Average Loss: 6.8078, epsilon: 5.78599\n",
      "update target\n",
      "Memory refreshed / sorted\n",
      "memory refresh\n",
      "Episode: 119/600, survival time: 25, Average Loss: 6.9988, epsilon: 5.14577\n",
      "Episode: 120/600, survival time: 18, Average Loss: 8.3661, epsilon: 4.72318\n",
      "Episode: 121/600, survival time: 16, Average Loss: 6.1114, epsilon: 4.37458\n",
      "Episode: 122/600, survival time: 8, Average Loss: 7.2960, epsilon: 4.20056\n",
      "Episode: 123/600, survival time: 16, Average Loss: 5.9306, epsilon: 3.89053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-19 14:40:56,689] Trial 0 failed with parameters: {} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\timoo\\AppData\\Local\\Temp\\ipykernel_17120\\1873511191.py\", line 168, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, writer), n_trials=1)  # Adjust the number of trials\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\timoo\\AppData\\Local\\Temp\\ipykernel_17120\\1873511191.py\", line 105, in objective\n",
      "    loss, originals = agent.replay(batch_size=batch_size, optimizer=optimizer, beta=beta)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\timoo\\AppData\\Local\\Temp\\ipykernel_17120\\946891037.py\", line 209, in replay\n",
      "    loss.backward()\n",
      "  File \"C:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\torch\\_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"C:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2023-12-19 14:40:56,692] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 168\u001b[0m\n\u001b[0;32m    166\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# study.optimize(objective, n_trials=10)  # You can adjust the number of trials\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust the number of trials\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Close TensorBoard writer\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# writer.close()\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters found during optimization\u001b[39;00m\n\u001b[0;32m    174\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32mC:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mC:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mC:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mC:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[9], line 168\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    166\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# study.optimize(objective, n_trials=10)  # You can adjust the number of trials\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust the number of trials\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Close TensorBoard writer\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# writer.close()\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters found during optimization\u001b[39;00m\n\u001b[0;32m    174\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "Cell \u001b[1;32mIn[9], line 105\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, writer)\u001b[0m\n\u001b[0;32m     95\u001b[0m   agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mremember(\n\u001b[0;32m     96\u001b[0m       observation\u001b[38;5;241m=\u001b[39mobservation,\n\u001b[0;32m     97\u001b[0m       action\u001b[38;5;241m=\u001b[39maction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m       tde\u001b[38;5;241m=\u001b[39mtde,\n\u001b[0;32m    102\u001b[0m       auxiliary_data \u001b[38;5;241m=\u001b[39m state)\n\u001b[0;32m    103\u001b[0m   writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTDError\u001b[39m\u001b[38;5;124m'\u001b[39m, tde, global_step)  \u001b[38;5;66;03m# Log the loss for each episode\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m loss, originals \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m   total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[1;32mIn[8], line 209\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size, optimizer, beta)\u001b[0m\n\u001b[0;32m    206\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_q\n\u001b[0;32m    208\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 209\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\School\\Semester 6 AI\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial, writer):\n",
    "\n",
    "    NUM_EPISODES = 600\n",
    "\n",
    "    # env = gym.make('CartPole-v1', new_step_api=True)\n",
    "    if True:\n",
    "      env = gym.make(\"CartPole-v1\",  render_mode=\"rgb_array\")\n",
    "    else:\n",
    "      env = gym.make('CartPole-v1')\n",
    "\n",
    "    image_size = (4,84,84)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Define hyperparameters to search\n",
    "    epsilon = 20\n",
    "    epsilon_min = 1 # trial.suggest_float('epsilon_min', 1e-3, 1e-1, log=True)\n",
    "    epsilon_decay = 0.9955 # trial.suggest_float('epsilon_decay',0.98, 0.99, log=True)\n",
    "    gamma = 0.95 # trial.suggest_float('gamma',0.9, 1, log=True)\n",
    "\n",
    "\n",
    "\n",
    "    lr = 0.0025\n",
    "    batch_size = 64\n",
    "    beta = 0.8\n",
    "\n",
    "    # Initialize your DQNAgent with the required parameters\n",
    "    agent = DQNAgent(\n",
    "        image_size=image_size,\n",
    "        action_size=action_size,\n",
    "        minlen=2048,\n",
    "        maxlen=2048,\n",
    "        epsilon=epsilon,\n",
    "        epsilon_min=epsilon_min,\n",
    "        epsilon_decay=epsilon_decay,\n",
    "        gamma=gamma\n",
    "    )\n",
    "\n",
    "    if False:\n",
    "      # Assuming you have the path to your saved model\n",
    "      saved_model_path = '/content/drive/My Drive/best_model.pth'\n",
    "\n",
    "      # Load the saved model state dict\n",
    "      best_model_state_dict = torch.load(saved_model_path)\n",
    "\n",
    "      # Set the loaded state dict to your agent's model\n",
    "      agent.model.load_state_dict(best_model_state_dict)\n",
    "      agent.target_model.load_state_dict(best_model_state_dict)  # If needed for target model\n",
    "\n",
    "      # Ensure evaluation mode for inference\n",
    "      agent.target_model.eval()  # If needed for target model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Define optimizer with suggested learning rate\n",
    "    optimizer = optim.AdamW(agent.model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    best_episode = 0\n",
    "    best_time = 0\n",
    "    T = np.zeros(NUM_EPISODES)\n",
    "    global_step = 0\n",
    "    # Training loop\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        total_loss = 0  # Initialize loss for each episode\n",
    "        N= 0\n",
    "\n",
    "        # Perform training steps (agent.replay(), etc.) here\n",
    "        state = env.reset()\n",
    "        frame = env.render()\n",
    "\n",
    "        agent.frame_handler.initialize_frame_stack(frame)\n",
    "        observation = agent.frame_handler.convert_frame_stack_to_tensor()\n",
    "\n",
    "        for time in range(500):\n",
    "\n",
    "            action = agent.act(observation)\n",
    "\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if terminated:\n",
    "              reward = reward - 10\n",
    "\n",
    "            next_frame = env.render()\n",
    "            agent.frame_handler.update_framestack(next_frame)\n",
    "            next_observation = agent.frame_handler.convert_frame_stack_to_tensor()\n",
    "\n",
    "            # reward = custom_reward(next_state)\n",
    "\n",
    "            if time > 3:\n",
    "              tde = agent.td_error(observation, action, reward, next_observation, done)\n",
    "              agent.memory.remember(\n",
    "                  observation=observation,\n",
    "                  action=action,\n",
    "                  reward=reward,\n",
    "                  next_observation=next_observation,\n",
    "                  done=done,\n",
    "                  tde=tde,\n",
    "                  auxiliary_data = state)\n",
    "              writer.add_scalar('TDError', tde, global_step)  # Log the loss for each episode\n",
    "\n",
    "            loss, originals = agent.replay(batch_size=batch_size, optimizer=optimizer, beta=beta)\n",
    "\n",
    "\n",
    "\n",
    "            if loss is not None:\n",
    "              total_loss += loss\n",
    "              N+=1\n",
    "              writer.add_scalar('Loss', loss, global_step)  # Log the loss for each episode\n",
    "\n",
    "              flat_originals = originals.view(-1)  # Flatten the tensor\n",
    "              median_originals = torch.median(flat_originals)\n",
    "              quantiles = torch.quantile(flat_originals, torch.tensor([0.25, 0.5, 0.75]))\n",
    "\n",
    "              # Log median and quantiles using SummaryWriter\n",
    "              writer.add_scalar('Q_values/Quantile_25', quantiles[0], global_step)\n",
    "              writer.add_scalar('Q_values/Quantile_50', quantiles[1], global_step)\n",
    "              writer.add_scalar('Q_values/Quantile_75', quantiles[2], global_step)\n",
    "\n",
    "\n",
    "\n",
    "            observation = next_observation.clone()\n",
    "            state = new_state\n",
    "            global_step+= 1\n",
    "\n",
    "            if done:\n",
    "              break\n",
    "\n",
    "\n",
    "        # Log values to TensorBoard\n",
    "        writer.add_scalar('Epsilon', agent.epsilon, global_step)\n",
    "        writer.add_scalar('Time', time, global_step)\n",
    "        writer.add_scalar('espisode', episode, global_step)\n",
    "\n",
    "        T[episode] = time\n",
    "\n",
    "        if N!= 0:\n",
    "            average_loss = total_loss / N  # Calculate average loss per episode\n",
    "        else:\n",
    "            average_loss = np.inf\n",
    "\n",
    "        print(f\"Episode: {episode + 1}/{NUM_EPISODES}, survival time: {time:.0f}, Average Loss: {average_loss:.4f}, epsilon: {agent.epsilon:.5f}\")\n",
    "\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "    trial_data[trial.number] = {\n",
    "        'model_state_dict': agent.model.state_dict().copy(),\n",
    "    }\n",
    "    # Return the value to minimize (average loss)\n",
    "    return -np.percentile(T, 90)-np.max(T)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "trial_data = {}\n",
    "\n",
    "\n",
    "# Create a study object and optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=10)  # You can adjust the number of trials\n",
    "study.optimize(lambda trial: objective(trial, writer), n_trials=1)  # Adjust the number of trials\n",
    "\n",
    "# Close TensorBoard writer\n",
    "# writer.close()\n",
    "\n",
    "# Get the best hyperparameters found during optimization\n",
    "best_params = study.best_params\n",
    "print('Best hyperparameters:', best_params)\n",
    "\n",
    "# Find the best trial based on your objective function\n",
    "best_trial_number = study.best_trial.number\n",
    "\n",
    "# Retrieve data for the best trial\n",
    "best_trial_data = trial_data[best_trial_number]\n",
    "\n",
    "# Save the model state dictionary of the best trial\n",
    "best_model_state_dict = best_trial_data['model_state_dict']\n",
    "torch.save(best_model_state_dict, '/content/drive/My Drive/best_model.pth')\n",
    "\n",
    "writer.close()\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T13:40:57.843293300Z",
     "start_time": "2023-12-19T13:40:57.843293300Z"
    },
    "id": "Rd8c_ga6L-n0"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
